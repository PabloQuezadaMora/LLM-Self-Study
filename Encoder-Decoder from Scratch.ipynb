{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from keras import layers, activations\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "text = \"time flies like an arrow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  time flies like an arrow\n",
      "text_ids:  tf.Tensor([[  101  2051 10029  2066  2019  8612   102]], shape=(1, 7), dtype=int32)\n",
      "tokens:  ['[CLS]', 'time', 'flies', 'like', 'an', 'arrow', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(text, add_special_tokens=True, return_tensors = \"tf\") #tokenizamos nuestro texto\n",
    "print(\"text: \", text)\n",
    "print(\"text_ids: \", inputs.input_ids)\n",
    "print(\"tokens: \", tokenizer.convert_ids_to_tokens(inputs.input_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[101]], dtype=int32)>"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([[101]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim:  30522 output_dim:  768\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(model_ckpt) #Cargamos la config standard\n",
    "token_emb = layers.Embedding(config.vocab_size, config.hidden_size) #Cargamos el embedding con el input_dim y output_dim requeridos\n",
    "print(\"input_dim: \", token_emb.input_dim, \"output_dim: \", token_emb.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    dim_k = Q.shape[-1]\n",
    "    scores = tf.matmul(Q, tf.transpose(K, perm=[0, 2, 1])) / sqrt(dim_k)\n",
    "    weights = tf.nn.softmax(scores, axis = -1)\n",
    "    attn = tf.matmul(weights,V)\n",
    "    return attn\n",
    "\n",
    "class AttentionHead(layers.Layer):\n",
    "    def __init__(self, head_dim):\n",
    "        super().__init__()\n",
    "        self.Q = layers.Dense(head_dim) #Proyectamos nuestro hidden_state a head_dim dimensiones\n",
    "        self.K = layers.Dense(head_dim) #Proyectamos nuestro hidden_state a head_dim dimensiones\n",
    "        self.V = layers.Dense(head_dim) #Proyectamos nuestro hidden_state a head_dim dimensiones\n",
    "    \n",
    "    def call(self, hidden_state):\n",
    "        attn_outputs = scaled_dot_product_attention(self.Q(hidden_state), self.K(hidden_state), self.V(hidden_state)) #Calculamos Self-Attention\n",
    "        return attn_outputs\n",
    "    \n",
    "class MultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "        num_heads = config.num_attention_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.heads = [AttentionHead(head_dim) for _ in range(num_heads)]\n",
    "        self.output_linear = layers.Dense(embed_dim)\n",
    "        \n",
    "    def call(self, hidden_state):\n",
    "        x = tf.concat([h(hidden_state) for h in self.heads], axis = -1)\n",
    "        x = self.output_linear(x)\n",
    "        return x\n",
    "    \n",
    "class FeedForward(layers.Layer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "        intermediate_size = config.intermediate_size\n",
    "        self.linear_1 = layers.Dense(intermediate_size)\n",
    "        self.linear_2 = layers.Dense(embed_dim)\n",
    "        self.gelu = activations.gelu\n",
    "        dropout_rate = config.hidden_dropout_prob\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(layers.Layer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = layers.LayerNormalization()\n",
    "        self.layer_norm_2 = layers.LayerNormalization()\n",
    "        self.multi_head = MultiHeadAttention(config)\n",
    "        self.feedforward = FeedForward(config)\n",
    "    \n",
    "    def call(self, x):\n",
    "        hidden_state = self.layer_norm_1(x)\n",
    "        hidden_state = self.multi_head(hidden_state)\n",
    "        x = hidden_state + x\n",
    "        hidden_state = self.layer_norm_2(x)\n",
    "        hidden_state = self.feedforward(hidden_state)\n",
    "        x = hidden_state + x\n",
    "        return x\n",
    "    \n",
    "class Embeddings(layers.Layer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        vocab_size = config.vocab_size\n",
    "        embed_dim = config.hidden_size\n",
    "        self.token_embedding = layers.Embedding(vocab_size, embed_dim)\n",
    "        self.positional_embedding = layers.Embedding(config.max_position_embeddings, embed_dim)\n",
    "        self.layer_norm = layers.LayerNormalization()\n",
    "        self.dropout = layers.Dropout(config.hidden_dropout_prob)\n",
    "    \n",
    "    def call(self, input_ids):\n",
    "        input_ids = tf.convert_to_tensor(input_ids)\n",
    "        seq_length = input_ids.shape[0]\n",
    "        position_ids = tf.range(seq_length, dtype=tf.int32)\n",
    "        position_ids = tf.expand_dims(position_ids, axis=0)\n",
    "        token_emb = self.token_embedding(input_ids)\n",
    "        pos_emb = self.positional_embedding(position_ids)\n",
    "        embedding = token_emb + pos_emb\n",
    "        embedding = self.layer_norm(embedding)\n",
    "        embedding = self.dropout(embedding)   \n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer): \n",
    "    def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.embeddings = Embeddings(config)\n",
    "            self.layers = [TransformerEncoderLayer(config) for _ in range(config.num_hidden_layers)]\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embeddings(x) \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  (1, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 7, 768])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = TransformerEncoder(config)\n",
    "print(\"input: \", inputs.input_ids.shape)\n",
    "encoder(inputs.input_ids).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para implementar un Decoder, necesitamos aplicar un mask en el attention, pues solo dependemos del token actual y los anteriores al token. Para esto, consideremos una matriz mask con 0 arriba de la diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention_with_mask(Q, K, V, mask = None): #Actualizado para considerar el caso con mask\n",
    "    dim_k = tf.cast(Q.shape[-1], tf.float32)\n",
    "    scores = tf.matmul(Q, tf.transpose(K, perm=[0, 2, 1])) / tf.sqrt(dim_k)\n",
    "    seq_len = tf.shape(scores)[1]\n",
    "    if mask is not None:\n",
    "        mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        mask = tf.reshape(mask, (1, seq_len, seq_len))  # Add batch and head dimensions\n",
    "        scores = tf.where(mask == 0, -float(\"inf\"), scores)\n",
    "    weights = tf.nn.softmax(scores, axis = -1)\n",
    "    return tf.matmul(weights, V)\n",
    "\n",
    "class MaskedSelfAttention(layers.Layer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "        num_heads = config.num_attention_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.Q = layers.Dense(head_dim)\n",
    "        self.K = layers.Dense(head_dim) \n",
    "        self.V = layers.Dense(head_dim)\n",
    "        \n",
    "    def call(self, hidden_state, mask):\n",
    "        attn_outputs = scaled_dot_product_attention_with_mask(self.Q(hidden_state), self.K(hidden_state), self.V(hidden_state), mask)\n",
    "        return attn_outputs\n",
    "\n",
    "class MultiHeadAttentionMasked(layers.Layer):\n",
    "    def __init__(self, config, mask):\n",
    "        super().__init__()\n",
    "        self.mask = mask\n",
    "        embed_dim = config.hidden_size\n",
    "        num_heads = config.num_attention_heads\n",
    "        self.heads = [MaskedSelfAttention(config) for _ in range(num_heads)]\n",
    "        self.output_linear = layers.Dense(embed_dim)\n",
    "        \n",
    "    def call(self, hidden_state):\n",
    "        j = self.heads[0]\n",
    "        attention_outputs = [h(hidden_state, self.mask) for h in self.heads]\n",
    "        x = tf.concat(attention_outputs, axis=-1)\n",
    "        x = self.output_linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_decoder_attention(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "        num_heads = config.num_attention_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.Q = layers.Dense(head_dim)\n",
    "        self.K = layers.Dense(head_dim) \n",
    "        self.V = layers.Dense(head_dim)\n",
    "    \n",
    "    def call(self, mid_repr, encoder_key, encoder_value):\n",
    "        attn_outputs = scaled_dot_product_attention(self.Q(mid_repr), self.K(encoder_key), self.V(encoder_value)) #Calculamos Self-Attention\n",
    "        return attn_outputs\n",
    "\n",
    "class MultiHeadEncoderDecoderAttention(layers.Layer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "        num_heads = config.num_attention_heads\n",
    "        self.heads = [Encoder_decoder_attention() for _ in range(num_heads)]\n",
    "        self.output_linear = layers.Dense(embed_dim)\n",
    "        \n",
    "    def call(self, mid_repr, encoder_key, encoder_value):\n",
    "        attention = tf.concat([h(mid_repr, encoder_key, encoder_value) for h in self.heads], axis = -1)\n",
    "        attention = self.output_linear(attention)\n",
    "        return attention\n",
    "    \n",
    "class TransformerDecoderLayer(layers.Layer):\n",
    "    def __init__(self, config, mask):\n",
    "        super().__init__()\n",
    "        self.mask = mask\n",
    "        self.layer_norm_1 = layers.LayerNormalization()\n",
    "        self.layer_norm_2 = layers.LayerNormalization()\n",
    "        self.multi_head_masked = MultiHeadAttentionMasked(config, mask)\n",
    "        self.encoder_decoder_attention = MultiHeadEncoderDecoderAttention(config)\n",
    "        self.feedforward = FeedForward(config)\n",
    "    \n",
    "    def call(self, x, encoder_key, encoder_value):\n",
    "        hidden_state = self.layer_norm_1(x)\n",
    "        hidden_state = self.multi_head_masked(hidden_state)\n",
    "        x = hidden_state + x\n",
    "        hidden_state = self.layer_norm_2(x)\n",
    "        hidden_state = self.encoder_decoder_attention(hidden_state, encoder_key, encoder_value)\n",
    "        hidden_state = self.feedforward(hidden_state)\n",
    "        x = hidden_state + x\n",
    "        return x\n",
    "    \n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, config, mask):\n",
    "        super().__init__()\n",
    "        self.mask = mask\n",
    "        self.embeddings = Embeddings(config)\n",
    "        self.layers = [TransformerDecoderLayer(config, mask) for _ in range(config.num_hidden_layers)]\n",
    "\n",
    "    def call(self, x, encoder_key, encoder_value):\n",
    "        x = self.embeddings(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_key, encoder_value)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "seq_len = 9\n",
    "d_model = config.hidden_size\n",
    "\n",
    "x = tf.random.uniform((batch_size, seq_len))\n",
    "encoder_key = tf.random.uniform((batch_size, seq_len, 768))\n",
    "encoder_value = tf.random.uniform((batch_size, seq_len, 768))\n",
    "\n",
    "decoder = TransformerDecoder(config, mask=None)\n",
    "output = decoder(x, encoder_key, encoder_value)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(layers.Layer):\n",
    "    def __init__(self, config, mask):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(config)\n",
    "        self.decoder = TransformerDecoder(config, mask)\n",
    "\n",
    "    def run(self, x):\n",
    "        encoder_out = self.encoder(x)\n",
    "        print(\"0\", encoder_out.shape)\n",
    "        decoder_out =  self.decoder(tf.constant([[101]]), encoder_out, encoder_out)\n",
    "        return decoder_out\n",
    "    \n",
    "    def call(self, x):\n",
    "        return self.run(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  (1, 7)\n",
      "0 (1, 7, 768)\n",
      "1 (1, 1, 768)\n",
      "2 (1, 1, 768)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "a:  (1, 1, 64)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "b:  (1, 1, 768)\n",
      "3 (1, 1, 768)\n",
      "4 (1, 1, 768)\n",
      "5 (1, 1, 768)\n",
      "(1, 1, 64)\n",
      "6 (1, 1, 768)\n",
      "7 (1, 1, 768)\n",
      "1 (1, 1, 768)\n",
      "2 (1, 1, 768)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "a:  (1, 1, 64)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "b:  (1, 1, 768)\n",
      "3 (1, 1, 768)\n",
      "4 (1, 1, 768)\n",
      "5 (1, 1, 768)\n",
      "(1, 1, 64)\n",
      "6 (1, 1, 768)\n",
      "7 (1, 1, 768)\n",
      "1 (1, 1, 768)\n",
      "2 (1, 1, 768)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "a:  (1, 1, 64)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "b:  (1, 1, 768)\n",
      "3 (1, 1, 768)\n",
      "4 (1, 1, 768)\n",
      "5 (1, 1, 768)\n",
      "(1, 1, 64)\n",
      "6 (1, 1, 768)\n",
      "7 (1, 1, 768)\n",
      "1 (1, 1, 768)\n",
      "2 (1, 1, 768)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "a:  (1, 1, 64)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "b:  (1, 1, 768)\n",
      "3 (1, 1, 768)\n",
      "4 (1, 1, 768)\n",
      "5 (1, 1, 768)\n",
      "(1, 1, 64)\n",
      "6 (1, 1, 768)\n",
      "7 (1, 1, 768)\n",
      "1 (1, 1, 768)\n",
      "2 (1, 1, 768)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "a:  (1, 1, 64)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "b:  (1, 1, 768)\n",
      "3 (1, 1, 768)\n",
      "4 (1, 1, 768)\n",
      "5 (1, 1, 768)\n",
      "(1, 1, 64)\n",
      "6 (1, 1, 768)\n",
      "7 (1, 1, 768)\n",
      "1 (1, 1, 768)\n",
      "2 (1, 1, 768)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "a:  (1, 1, 64)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "b:  (1, 1, 768)\n",
      "3 (1, 1, 768)\n",
      "4 (1, 1, 768)\n",
      "5 (1, 1, 768)\n",
      "(1, 1, 64)\n",
      "6 (1, 1, 768)\n",
      "7 (1, 1, 768)\n",
      "1 (1, 1, 768)\n",
      "2 (1, 1, 768)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "a:  (1, 1, 64)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "b:  (1, 1, 768)\n",
      "3 (1, 1, 768)\n",
      "4 (1, 1, 768)\n",
      "5 (1, 1, 768)\n",
      "(1, 1, 64)\n",
      "6 (1, 1, 768)\n",
      "7 (1, 1, 768)\n",
      "1 (1, 1, 768)\n",
      "2 (1, 1, 768)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "a:  (1, 1, 64)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "b:  (1, 1, 768)\n",
      "3 (1, 1, 768)\n",
      "4 (1, 1, 768)\n",
      "5 (1, 1, 768)\n",
      "(1, 1, 64)\n",
      "6 (1, 1, 768)\n",
      "7 (1, 1, 768)\n",
      "1 (1, 1, 768)\n",
      "2 (1, 1, 768)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "a:  (1, 1, 64)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "b:  (1, 1, 768)\n",
      "3 (1, 1, 768)\n",
      "4 (1, 1, 768)\n",
      "5 (1, 1, 768)\n",
      "(1, 1, 64)\n",
      "6 (1, 1, 768)\n",
      "7 (1, 1, 768)\n",
      "1 (1, 1, 768)\n",
      "2 (1, 1, 768)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "a:  (1, 1, 64)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "b:  (1, 1, 768)\n",
      "3 (1, 1, 768)\n",
      "4 (1, 1, 768)\n",
      "5 (1, 1, 768)\n",
      "(1, 1, 64)\n",
      "6 (1, 1, 768)\n",
      "7 (1, 1, 768)\n",
      "1 (1, 1, 768)\n",
      "2 (1, 1, 768)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "a:  (1, 1, 64)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "b:  (1, 1, 768)\n",
      "3 (1, 1, 768)\n",
      "4 (1, 1, 768)\n",
      "5 (1, 1, 768)\n",
      "(1, 1, 64)\n",
      "6 (1, 1, 768)\n",
      "7 (1, 1, 768)\n",
      "1 (1, 1, 768)\n",
      "2 (1, 1, 768)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "a:  (1, 1, 64)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "Q:  (1, 1, 64)\n",
      "K:  (1, 1, 64)\n",
      "V: (1, 1, 64)\n",
      "scores: (1, 1, 1)\n",
      "weights:  (1, 1, 1)\n",
      "b:  (1, 1, 768)\n",
      "3 (1, 1, 768)\n",
      "4 (1, 1, 768)\n",
      "5 (1, 1, 768)\n",
      "(1, 1, 64)\n",
      "6 (1, 1, 768)\n",
      "7 (1, 1, 768)\n",
      "total output:  (1, 1, 768)\n"
     ]
    }
   ],
   "source": [
    "mask = True\n",
    "encoder_decoder = EncoderDecoder(config, mask)\n",
    "x = tf.random.uniform((1, 7))\n",
    "print(\"input: \", x.shape)\n",
    "print(\"total output: \", encoder_decoder(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solo nos falta ponerle el head a nuestro encoder-decoder model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
