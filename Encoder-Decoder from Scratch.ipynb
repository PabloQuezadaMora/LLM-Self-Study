{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from keras import layers, activations\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos un Tokenizer ya entrenado. En este caso usaremos Bert que agrega caracteres especiales como [CLS] y [SEP]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"time flies like an arrow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para observar como funciona el tokenizer, pasaremos el text \"time flies like an arrow\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  time flies like an arrow\n",
      "text_ids:  tf.Tensor([[  101  2051 10029  2066  2019  8612   102]], shape=(1, 7), dtype=int32)\n",
      "tokens:  ['[CLS]', 'time', 'flies', 'like', 'an', 'arrow', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(text, add_special_tokens=True, return_tensors = \"tf\") #tokenizamos nuestro texto\n",
    "print(\"text: \", text)\n",
    "print(\"text_ids: \", inputs.input_ids)\n",
    "print(\"tokens: \", tokenizer.convert_ids_to_tokens(inputs.input_ids[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomemos la configuración usual del modelo \"bert-base-uncased\" usado para definir nuestro Encoder y Decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim:  30522 output_dim:  768\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\"bert-base-uncased\") #Cargamos la config standard\n",
    "token_emb = layers.Embedding(config.vocab_size, config.hidden_size) #Cargamos el embedding con el input_dim y output_dim requeridos\n",
    "print(\"input_dim: \", token_emb.input_dim, \"output_dim: \", token_emb.output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos notar que tiene un vocabulario de 30522 token (incluyendo tokens especiales), y representa cada token como un vector de 768 dimensiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezaremos definiendo el Encoder. Para esto, necesitamos definir:\n",
    "\n",
    "- Función que calcula el Self-Attention entre el query (Q), key (K) y value (V). Recordemos que\n",
    "\n",
    "$$ SelfAttention(Q, K ,V) = softmax\\left(\\frac{Q\\cdot K^t}{\\sqrt{dim_k}}\\right)\\cdot V $$\n",
    "\n",
    "En este caso no estamos utilizando un masking para el Self Attention.\n",
    "\n",
    "- Attention que proyecta nuestro hidden_state para luego calcular el Self-attention. Tiene parametros $Q$, $K$ y $V$. \n",
    "\n",
    "- Teniendo el attentionhead, definimos el Multi Attention Head, que aplica una cantidad de Attention para luego concatenar el resultado y proyectarlo a la dimensión deseada.\n",
    "\n",
    "- Finalmente, definimos una layer Feed Forward consistente de proyectar, aplicar GELU, proyectar y luego un Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V): #Dados Q, K y V, calcula el self-attention\n",
    "    dim_k = tf.cast(Q.shape[-1], tf.float32)\n",
    "    scores = tf.matmul(Q, tf.transpose(K, perm=[0, 2, 1])) / tf.sqrt(dim_k) \n",
    "    weights = tf.nn.softmax(scores, axis = -1)\n",
    "    attn = tf.matmul(weights,V)\n",
    "    return attn\n",
    "\n",
    "class AttentionHead(layers.Layer): #Dado un hidden state, utiliza sus propios Q, K y V para proyectar y luego calcular el self-attention.\n",
    "    def __init__(self, head_dim):\n",
    "        super().__init__()\n",
    "        self.Q = layers.Dense(head_dim) #Proyectamos nuestro hidden_state a head_dim dimensiones\n",
    "        self.K = layers.Dense(head_dim) \n",
    "        self.V = layers.Dense(head_dim) \n",
    "        \n",
    "    def call(self, hidden_state):\n",
    "        attn_outputs = scaled_dot_product_attention(self.Q(hidden_state), self.K(hidden_state), self.V(hidden_state)) #Calculamos Self-Attention de lo proyectado\n",
    "        return attn_outputs\n",
    "    \n",
    "class MultiHeadAttention(layers.Layer): #Crea varios AttentionHead para crear un Multi-Self-Attention \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "        num_heads = config.num_attention_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.heads = [AttentionHead(head_dim) for _ in range(num_heads)]\n",
    "        self.output_linear = layers.Dense(embed_dim)\n",
    "        \n",
    "    def call(self, hidden_state):\n",
    "        x = tf.concat([h(hidden_state) for h in self.heads], axis = -1)\n",
    "        x = self.output_linear(x)\n",
    "        return x\n",
    "    \n",
    "class FeedForward(layers.Layer): #Un FF Layer.\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "        intermediate_size = config.intermediate_size\n",
    "        self.linear_1 = layers.Dense(intermediate_size)\n",
    "        self.linear_2 = layers.Dense(embed_dim)\n",
    "        self.gelu = activations.gelu\n",
    "        dropout_rate = config.hidden_dropout_prob\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto ya podemos definir nuestro Layer en que consite el Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(layers.Layer): #Nuestro Encoder\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = layers.LayerNormalization()\n",
    "        self.layer_norm_2 = layers.LayerNormalization()\n",
    "        self.multi_head = MultiHeadAttention(config)\n",
    "        self.feedforward = FeedForward(config)\n",
    "    \n",
    "    def call(self, x):\n",
    "        hidden_state = self.layer_norm_1(x)\n",
    "        hidden_state = self.multi_head(hidden_state)\n",
    "        x = hidden_state + x\n",
    "        hidden_state = self.layer_norm_2(x)\n",
    "        hidden_state = self.feedforward(hidden_state)\n",
    "        x = hidden_state + x\n",
    "        return x\n",
    "    \n",
    "class Embeddings(layers.Layer): #Definimos también un Embedding que codifica tanto nuestros tokens como su posición en la frase\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        vocab_size = config.vocab_size\n",
    "        embed_dim = config.hidden_size\n",
    "        self.token_embedding = layers.Embedding(vocab_size, embed_dim)\n",
    "        self.positional_embedding = layers.Embedding(config.max_position_embeddings, embed_dim)\n",
    "        self.layer_norm = layers.LayerNormalization()\n",
    "        self.dropout = layers.Dropout(config.hidden_dropout_prob)\n",
    "    \n",
    "    def call(self, input_ids):\n",
    "        seq_length = input_ids.shape[0]\n",
    "        position_ids = tf.range(seq_length, dtype=tf.int32)\n",
    "        position_ids = tf.expand_dims(position_ids, axis=0)\n",
    "        token_emb = self.token_embedding(input_ids)\n",
    "        pos_emb = self.positional_embedding(position_ids)\n",
    "        embedding = token_emb + pos_emb\n",
    "        embedding = self.layer_norm(embedding)\n",
    "        embedding = self.dropout(embedding)   \n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniendo ya todos los componente, definimos el Encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer): \n",
    "    def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.embeddings = Embeddings(config) #Hacemos embedding\n",
    "            self.layers = [TransformerEncoderLayer(config) for _ in range(config.num_hidden_layers)] #Luego lo pasamos por una cierta cantidad de EncoderLayers\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embeddings(x) \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que el Encoder toma un input de la forma (batch_size, length), y nos entrega un tensor con forma (batch_size, lenght, dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size:  (1, 7)\n",
      "output size:  (1, 7, 768)\n"
     ]
    }
   ],
   "source": [
    "encoder = TransformerEncoder(config)\n",
    "print(\"input size: \", inputs.input_ids.shape)\n",
    "print(\"output size: \", encoder(inputs.input_ids).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para implementar un Decoder, necesitamos aplicar un mask en el attention, pues solo dependeremos del token actual y los anteriores al token para hacer nuestra predicción. Para esto, consideremos una matriz mask con 0 arriba de la diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention_with_mask(Q, K, V, mask = None): #Actualizado para considerar el caso con mask\n",
    "    dim_k = tf.cast(Q.shape[-1], tf.float32)\n",
    "    scores = tf.matmul(Q, tf.transpose(K, perm=[0, 2, 1])) / tf.sqrt(dim_k)\n",
    "    seq_len = tf.shape(scores)[1]\n",
    "    if mask is not None:\n",
    "        mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        mask = tf.reshape(mask, (1, seq_len, seq_len))  # Add batch and head dimensions\n",
    "        scores = tf.where(mask == 0, -float(\"inf\"), scores)\n",
    "    weights = tf.nn.softmax(scores, axis = -1)\n",
    "    return tf.matmul(weights, V)\n",
    "\n",
    "class MaskedSelfAttention(layers.Layer): #SelfAttention con el Masked product.\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "        num_heads = config.num_attention_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.Q = layers.Dense(head_dim)\n",
    "        self.K = layers.Dense(head_dim) \n",
    "        self.V = layers.Dense(head_dim)\n",
    "        \n",
    "    def call(self, hidden_state, mask):\n",
    "        attn_outputs = scaled_dot_product_attention_with_mask(self.Q(hidden_state), self.K(hidden_state), self.V(hidden_state), mask)\n",
    "        return attn_outputs\n",
    "\n",
    "class MultiHeadAttentionMasked(layers.Layer): #Multi Self-Attention con el masked product.\n",
    "    def __init__(self, config, mask):\n",
    "        super().__init__()\n",
    "        self.mask = mask\n",
    "        embed_dim = config.hidden_size\n",
    "        num_heads = config.num_attention_heads\n",
    "        self.heads = [MaskedSelfAttention(config) for _ in range(num_heads)]\n",
    "        self.output_linear = layers.Dense(embed_dim)\n",
    "        \n",
    "    def call(self, hidden_state):\n",
    "        j = self.heads[0]\n",
    "        attention_outputs = [h(hidden_state, self.mask) for h in self.heads]\n",
    "        x = tf.concat(attention_outputs, axis=-1)\n",
    "        x = self.output_linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También queremos considerar el attention con el output del Encoder. Para esto, definimos el Encoder decoder attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_decoder_attention(layers.Layer): #Encoder decoder attention\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "        num_heads = config.num_attention_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.Q = layers.Dense(head_dim)\n",
    "        self.K = layers.Dense(head_dim) \n",
    "        self.V = layers.Dense(head_dim)\n",
    "    \n",
    "    def call(self, mid_repr, encoder_key, encoder_value):\n",
    "        attn_outputs = scaled_dot_product_attention(self.Q(mid_repr), self.K(encoder_key), self.V(encoder_value)) #Calculamos Self-Attention\n",
    "        return attn_outputs\n",
    "\n",
    "class MultiHeadEncoderDecoderAttention(layers.Layer): #Multi Encoder decoder attention\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "        num_heads = config.num_attention_heads\n",
    "        self.heads = [Encoder_decoder_attention() for _ in range(num_heads)]\n",
    "        self.output_linear = layers.Dense(embed_dim)\n",
    "        \n",
    "    def call(self, mid_repr, encoder_key, encoder_value):\n",
    "        attention = tf.concat([h(mid_repr, encoder_key, encoder_value) for h in self.heads], axis = -1)\n",
    "        attention = self.output_linear(attention)\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así,  ya tenemos todas las herramientas para construir el Layer que comprendrá nuestro Decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(layers.Layer):\n",
    "    def __init__(self, config, mask):\n",
    "        super().__init__()\n",
    "        self.mask = mask\n",
    "        self.layer_norm_1 = layers.LayerNormalization()\n",
    "        self.layer_norm_2 = layers.LayerNormalization()\n",
    "        self.multi_head_masked = MultiHeadAttentionMasked(config, mask)\n",
    "        self.encoder_decoder_attention = MultiHeadEncoderDecoderAttention(config)\n",
    "        self.feedforward = FeedForward(config)\n",
    "    \n",
    "    def call(self, x, encoder_key, encoder_value):\n",
    "        hidden_state = self.layer_norm_1(x)\n",
    "        hidden_state = self.multi_head_masked(hidden_state)\n",
    "        x = hidden_state + x\n",
    "        hidden_state = self.layer_norm_2(x)\n",
    "        hidden_state = self.encoder_decoder_attention(hidden_state, encoder_key, encoder_value)\n",
    "        hidden_state = self.feedforward(hidden_state)\n",
    "        x = hidden_state + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro Transformer será una aplicación sucesiva de estos layers. Más adelante agregaremos un classifcation head y un token prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, config, mask):\n",
    "        super().__init__()\n",
    "        self.mask = mask\n",
    "        self.embeddings = Embeddings(config)\n",
    "        self.layers = [TransformerDecoderLayer(config, mask) for _ in range(config.num_hidden_layers)]\n",
    "\n",
    "    def call(self, x, encoder_key, encoder_value):\n",
    "        x = self.embeddings(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_key, encoder_value)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testeamos que nuestro Decoder está entregando los resultados esperados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (1, 9)\n",
      "Output shape: (1, 9, 768)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "seq_len = 9\n",
    "d_model = config.hidden_size\n",
    "\n",
    "x = tf.random.uniform((batch_size, seq_len))\n",
    "encoder_key = tf.random.uniform((batch_size, seq_len, 768))\n",
    "encoder_value = tf.random.uniform((batch_size, seq_len, 768))\n",
    "\n",
    "decoder = TransformerDecoder(config, mask=None)\n",
    "output = decoder(x, encoder_key, encoder_value)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto podemos armar nuestro Encoder-Decoder transformer. Notemos que el output del encoder es evaluado dentro del decoder, pero el decoder tiene su propio input con el que empieza a generar tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(layers.Layer):\n",
    "    def __init__(self, config, mask):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(config)\n",
    "        self.decoder = TransformerDecoder(config, mask)\n",
    "\n",
    "    def run(self, x):\n",
    "        encoder_out = self.encoder(x)\n",
    "        decoder_out =  self.decoder(tf.constant([[101]]), encoder_out, encoder_out) #[101] corresponde al token [CLS] con que empieza un texto.\n",
    "        return decoder_out\n",
    "    \n",
    "    def call(self, x):\n",
    "        return self.run(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  (1, 7)\n",
      "total output:  (1, 1, 768)\n"
     ]
    }
   ],
   "source": [
    "mask = True\n",
    "encoder_decoder = EncoderDecoder(config, mask)\n",
    "x = tf.random.uniform((1, 7))\n",
    "print(\"input: \", x.shape)\n",
    "print(\"total output: \", encoder_decoder(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unembedding(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        vocab_size = config.vocab_size\n",
    "        self.unembedding = layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        return self.unembedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "unembedder = Unembedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['white']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tf.argmax(unembedder(x), axis = -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora agregaremos un prediction head a nuestro model, para poder crear un Encoder-Decoder que sea auto-regresivo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
